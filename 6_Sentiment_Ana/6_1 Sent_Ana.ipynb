{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Test data-(33052, 1)\n",
      "Shape of training data - (7086, 2)\n",
      "   Sentiment                                               Text\n",
      "0          1            The Da Vinci Code book is just awesome.\n",
      "1          1  this was the first clive cussler i've ever rea...\n",
      "2          1                   i liked the Da Vinci Code a lot.\n",
      "3          1                   i liked the Da Vinci Code a lot.\n",
      "4          1  I liked the Da Vinci Code but it ultimatly did...\n",
      "                                                Text\n",
      "0  \" I don't care what anyone says, I like Hillar...\n",
      "1                  have an awesome time at purdue!..\n",
      "2  Yep, I'm still in London, which is pretty awes...\n",
      "3  Have to say, I hate Paris Hilton's behavior bu...\n",
      "4                            i will love the lakers.\n",
      "No of labels we have for each sentiment class - \n",
      "1    3995\n",
      "0    3091\n",
      "Name: Sentiment, dtype: int64\n",
      "AVG no. of words per sentence - 10.8868190799\n",
      "(40138, 85)\n",
      "['aaa', 'amaz', 'angelina', 'awesom', 'beauti', 'becaus', 'boston', 'brokeback', 'citi', 'code', 'cool', 'cruis', 'd', 'da', 'drive', 'francisco', 'friend', 'fuck', 'geico', 'good', 'got', 'great', 'ha', 'harri', 'harvard', 'hate', 'hi', 'hilton', 'honda', 'imposs', 'joli', 'just', 'know', 'laker', 'left', 'like', 'littl', 'london', 'look', 'lot', 'love', 'm', 'macbook', 'make', 'miss', 'mission', 'mit', 'mountain', 'movi', 'need', 'new', 'oh', 'onli', 'pari', 'peopl', 'person', 'potter', 'purdu', 'realli', 'right', 'rock', 's', 'said', 'san', 'say', 'seattl', 'shanghai', 'stori', 'stupid', 'suck', 't', 'thi', 'thing', 'think', 'time', 'tom', 'toyota', 'ucla', 've', 'vinci', 'wa', 'want', 'way', 'whi', 'work']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "So in the sentiment analysis process there are a couple of stages more or less differentiated. The first is about \n",
    "processing natural language, and the second about training a model. The first stage is in charge of processing text in \n",
    "a way that, when we are ready to train our model, we already know what variables the model needs to consider as inputs. \n",
    "The model itself is in charge of learning how to determine the sentiment of a piece of text based on these variables. \n",
    "\n",
    "For the model part we will introduce and use linear models. They aren't the most powerful methods in terms of accuracy, \n",
    "but they are simple enough to be interpreted in their results as we will see. Linear methods allow us to define our input \n",
    "variable as a linear combination of input variables. In tis case we will introduce logistic regression.\n",
    "\n",
    "Finally we will need some data to train our model. For this we will use data from the Kaggle competition UMICH SI650. \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# define local file names\n",
    "test_data_file_name = '6_1_testdata.txt'\n",
    "train_data_file_name = '6_1_training.txt'\n",
    "#load files into data frames for processing.\n",
    "test_data_df = pd.read_csv(test_data_file_name, header=None, delimiter=\"\\t\", quoting=3)\n",
    "test_data_df.columns = [\"Text\"]\n",
    "train_data_df = pd.read_csv(train_data_file_name, header=None, delimiter=\"\\t\", quoting=3)\n",
    "train_data_df.columns = [\"Sentiment\",\"Text\"]\n",
    "print(\"Shape of Test data-\" + str(test_data_df.shape) + \"\\n\" + \"Shape of training data - \" + str(train_data_df.shape))\n",
    "print(train_data_df.head())\n",
    "print(test_data_df.head())\n",
    "print(\"No of labels we have for each sentiment class - \\n\" + str(train_data_df.Sentiment.value_counts()))\n",
    "print(\"AVG no. of words per sentence - \"+str(np.mean([len(s.split(\" \")) for s in train_data_df.Text])))\n",
    "\"\"\"\n",
    "We will process our text sentences and create a corpus. We will also extract important words and establish them as input \n",
    "variables for our classifier. We will use basic transformations.  The requirements of a bag-of-words classifier are minimal \n",
    ". We just need to count words, so the process is reduced to do some simplification and unification of terms  and then \n",
    "count them. The simplification process mostly includes removing punctuation, lowercasing, removing stop-words, and \n",
    "reducing words to its lexical roots (i.e. stemming).\n",
    "    The class sklearn.feature_extraction.text.CountVectorizer in the wonderful scikit learn Python library converts a \n",
    "collection of text documents to a matrix of token counts. This is just what we need to implement later on our bag-of-words \n",
    "linear classifier.\n",
    "    First we need to init the vectoriser. We need to remove punctuations, lowercase, remove stop words, and stem words. \n",
    "All these steps can be directly performed by CountVectorizer if we pass the right parameter values. We are using porter\n",
    "for stemming.\n",
    "    The approach we will be using here is called a bag-of-words model. In this kind of model we simplify documents to a\n",
    "multi-set of terms frequencies. That means that, for our model, a document sentiment tag will depend on what words appear \n",
    "in that document, discarding any grammar or word order but keeping multiplicity.This is what we just did before, use our text \n",
    "entries to build term frequencies. We ended up with the same entries in our dataset but, instead of having them defined by \n",
    "a whole text, they are now defined by a series of counts of the most frequent words in our whole corpus. Now we are going\n",
    "to use these vectors as features to train a classifier.\n",
    "\"\"\"\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # stem\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "########\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word', tokenizer = tokenize, lowercase = True, stop_words = 'english', max_features = 85 )\n",
    "corpus_data_features = vectorizer.fit_transform(train_data_df.Text.tolist() + test_data_df.Text.tolist())\n",
    "corpus_data_features_nd = corpus_data_features.toarray()\n",
    "print(corpus_data_features_nd.shape)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179:aaa\n",
      "485:amaz\n",
      "1765:angelina\n",
      "3170:awesom\n",
      "2146:beauti\n",
      "1694:becaus\n",
      "2190:boston\n",
      "2000:brokeback\n",
      "423:citi\n",
      "2003:code\n",
      "481:cool\n",
      "2031:cruis\n",
      "439:d\n",
      "2087:da\n",
      "433:drive\n",
      "1926:francisco\n",
      "477:friend\n",
      "452:fuck\n",
      "1085:geico\n",
      "773:good\n",
      "571:got\n",
      "1178:great\n",
      "776:ha\n",
      "2094:harri\n",
      "2103:harvard\n",
      "4492:hate\n",
      "794:hi\n",
      "2086:hilton\n",
      "2192:honda\n",
      "1098:imposs\n",
      "1764:joli\n",
      "1054:just\n",
      "896:know\n",
      "2019:laker\n",
      "425:left\n",
      "4080:like\n",
      "507:littl\n",
      "2233:london\n",
      "811:look\n",
      "421:lot\n",
      "10334:love\n",
      "1568:m\n",
      "1059:macbook\n",
      "631:make\n",
      "1098:miss\n",
      "1101:mission\n",
      "1340:mit\n",
      "2081:mountain\n",
      "1207:movi\n",
      "1220:need\n",
      "459:new\n",
      "551:oh\n",
      "674:onli\n",
      "2094:pari\n",
      "1018:peopl\n",
      "454:person\n",
      "2093:potter\n",
      "1167:purdu\n",
      "2126:realli\n",
      "661:right\n",
      "475:rock\n",
      "3914:s\n",
      "495:said\n",
      "2038:san\n",
      "627:say\n",
      "2019:seattl\n",
      "1189:shanghai\n",
      "467:stori\n",
      "2886:stupid\n",
      "4614:suck\n",
      "1455:t\n",
      "1705:thi\n",
      "662:thing\n",
      "1524:think\n",
      "781:time\n",
      "2117:tom\n",
      "2028:toyota\n",
      "2008:ucla\n",
      "774:ve\n",
      "2001:vinci\n",
      "3703:wa\n",
      "1656:want\n",
      "932:way\n",
      "547:whi\n",
      "512:work\n"
     ]
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(corpus_data_features_nd, axis=0)\n",
    "    \n",
    "# For each, print the vocabulary word and the number of times it  appears in the data set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print (str(count)+':'+str(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words Representation\n",
    "print(corpus_data_features_nd)\n",
    "#print(corpus_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to perform logistic regression in Python we use LogisticRegression. But first let's split our training data in order to get an evaluation set.\n",
    "\"\"\" \n",
    "#from sklearn.cross_validation import train_test_split  <<<DEPRECATED>>>\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Corpus_data_features_nd contains all of our original train and test data, so we need to exclude the unlabeled test entries\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        corpus_data_features_nd[0:len(train_data_df)], \n",
    "        train_data_df.Sentiment,\n",
    "        train_size=0.85, \n",
    "        random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98       467\n",
      "          1       0.99      0.98      0.99       596\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=X_train, y=y_train)\n",
    "y_pred = log_model.predict(X_test)\n",
    "# Train our classifier There is a function for classification called sklearn.metrics.classification_report which calculates several \n",
    "# types of (predictive) scores on a classification model. \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tAs much as I hate Tom Cruise..\n",
      "0\tAll I can say is Northwest airlines are a bunch of idiots as well as Delta and if at all possible I won't be flying with them anymore...\n",
      "1\ti miss AAA...\n",
      "1\tWell, my job at GEICO has been great so far.\n",
      "1\ti love mit and harvard both..\n",
      "0\toh look, it's a parody of those crappy mastercard ads..\n",
      "1\tI LOVE MY TOYOTA COROLLA S! Except...\n",
      "0\tTBS's new stuff sucks, AAA's stuff is boring, and I basically only like Graduation Day and Beating Heart Baby from Head Automatica...\n",
      "1\tI just switched to Allstate insurance, because I miss David \" Allstate \" Palmer..\n",
      "0\tBottomline American Airlines sucks and Jet Blue rocks.\n",
      "1\t\" I LOVE SHANGHAI VERY MUCH!\n",
      "0\tWhich makes me think I need AAA more than AA...\n",
      "1\tMy new desk is in a windowed corner with an awesome view of Seattle and the Olympic Mountains.\n",
      "1\tOh my god I LOVE Pommes mit Mayo.\n",
      "1\tmostly these stupid guys with their shitty honda civics trying to sup them up like they are badass.\n"
     ]
    }
   ],
   "source": [
    "# Finally, we can re-train our model with all the training data and use it for sentiment classification with the original (unlabeled) test set.\n",
    "# train classifier\n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=corpus_data_features_nd[0:len(train_data_df)], y=train_data_df.Sentiment)\n",
    "    \n",
    "# get predictions\n",
    "test_pred = log_model.predict(corpus_data_features_nd[len(train_data_df):])\n",
    "    \n",
    "# sample some of them\n",
    "import random\n",
    "spl = random.sample(range(len(test_pred)), 15)\n",
    "    \n",
    "# print text and labels\n",
    "for text, sentiment in zip(test_data_df.Text[spl], test_pred[spl]):\n",
    "    print(str(sentiment) +'\\t'+( text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
